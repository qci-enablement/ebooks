{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Feature Selection Approach using QCi's Dirac-1\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In machine learning problems, we often have to start with a large number of features. We need a feature selection technique that can discover a relatively small subset of the most relevant features. In what follows we present a tutorial on using QCi's technology to select a set of features by minimzing their inter-correlation. This approach can be used with any unsupervised machine learning approach such as anomaly detection and clustering algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Methodology\n",
    "\n",
    "Let us have a dataset with $N$ samples and $d$ features, represented by a $N \\times d$ matrix $F$. Moreover, let us each column of $F$ by $\\bf{f}_{j}$ where $j \\in {1,...,d}$, representing all samples for feature $j$. We can choose a subset features of size $d^\\prime$ ($d^\\prime < d$) such that the inter-correlation of the subset is minimal. We have\n",
    "\n",
    "$$\\min_{\\bf{x}} \\sum_{i=1}^{d} \\sum_{j=1}^{d} C_{ij} x_{i} x_{j}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$C_{ij} = |corr(\\bf{f_{i}}, \\bf{f_{j}})|$$\n",
    "\n",
    "where $corr$ denotes a correlation function such as the Pearson correlation, and $x_{i} \\in \\{0, 1\\}$ is a binary variable indicating inclusion or exclustion of feature $i$. Obviously, $C_{ii} = 1$. The above minimization problem is subject to a constraint,\n",
    "\n",
    "$$\\sum_{i=1}^{d} x_{i} = d^\\prime$$\n",
    "\n",
    "We can exclude the diagonal elements of $C$ as they always add up to\n",
    "$d^\\prime$. In the matrix form we have,\n",
    "\n",
    "$$\\min_{\\bf{x}} \\bf{{x}^{T}} (C - I) \\bf{{x}}$$\n",
    "\n",
    "subject to the above constraint. Note that $I$ is a $d \\times d$ identity matrix. Note too that we have assumed that the reduced dimension $d^\\prime$ is assumed to be given in the above approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medicare Prescriotion Data\n",
    "\n",
    "We implemented this approach using a publically available set of data on prescription of opioids in the United States. The dataset can be found at https://www.cms.gov/data-research/statistics-trends-and-reports/medicare-provider-utilization-payment-data/part-d-prescriber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We start by cleaning the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input                                                                                       \n",
    "INP_FILE = \"Medicare_Provider_Utilization_and_Payment_Data__Part_D_Prescriber_Summary_Table_CY2014__50001-NNN__ANON.csv\"\n",
    "OUT_FILE  = \"cleaned_medicare_data.csv\"\n",
    "\n",
    "CON_VARS = [\n",
    "    \"total_claim_count\",\n",
    "    \"total_30_day_fill_count\",\n",
    "    \"total_drug_cost\",\n",
    "    \"total_day_supply\",\n",
    "    \"bene_count\",\n",
    "    \"total_claim_count_ge65\",\n",
    "    \"total_30_day_fill_count_ge65\",\n",
    "    \"total_drug_cost_ge65\",\n",
    "    \"total_day_supply_ge65\",\n",
    "    \"bene_count_ge65\",\n",
    "    \"brand_claim_count\",\n",
    "    \"brand_drug_cost\",\n",
    "    \"generic_claim_count\",\n",
    "    \"generic_drug_cost\",\n",
    "    \"other_claim_count\",\n",
    "    \"other_drug_cost\",\n",
    "    \"mapd_claim_count\",\n",
    "    \"mapd_drug_cost\",\n",
    "    \"pdp_claim_count\",\n",
    "    \"pdp_drug_cost\",\n",
    "    \"lis_claim_count\",\n",
    "    \"lis_drug_cost\",\n",
    "    \"nonlis_claim_count\",\n",
    "    \"nonlis_drug_cost\",\n",
    "    \"opioid_claim_count\",\n",
    "    \"opioid_drug_cost\",\n",
    "    \"opioid_day_supply\",\n",
    "    \"opioid_bene_count\",\n",
    "    \"opioid_prescriber_rate\",\n",
    "    \"antibiotic_claim_count\",\n",
    "    \"antibiotic_drug_cost\",\n",
    "    \"antibiotic_bene_count\",\n",
    "    \"hrm_claim_count_ge65\",\n",
    "    \"hrm_drug_cost_ge65\",\n",
    "    \"hrm_bene_count_ge65\",\n",
    "    \"antipsych_claim_count_ge65\",\n",
    "    \"antipsych_drug_cost_ge65\",\n",
    "    \"antipsych_bene_count_ge65\",\n",
    "    \"average_age_of_beneficiaries\",\n",
    "    \"beneficiary_age_less_65_count\",\n",
    "    \"beneficiary_age_65_74_count\",\n",
    "    \"beneficiary_age_75_84_count\",\n",
    "    \"beneficiary_age_greater_84_count\",\n",
    "    \"beneficiary_female_count\",\n",
    "    \"beneficiary_male_count\",\n",
    "    \"beneficiary_race_white_count\",\n",
    "    \"beneficiary_race_black_count\",\n",
    "    \"beneficiary_race_asian_pi_count\",\n",
    "    \"beneficiary_race_hispanic_count\",\n",
    "    \"beneficiary_race_nat_ind_count\",\n",
    "    \"beneficiary_race_other_count\",\n",
    "    \"beneficiary_nondual_count\",\n",
    "    \"beneficiary_dual_count\",\n",
    "    \"beneficiary_average_risk_score\",\n",
    "]\n",
    "\n",
    "VALID_PROVIDER_MI = [\n",
    "    \"A\",\n",
    "    \"M\",\n",
    "    \"J\",\n",
    "    \"L\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"E\",\n",
    "    \"D\",\n",
    "    \"C\",\n",
    "    \"B\",\n",
    "    \"K\",\n",
    "    \"P\",\n",
    "    \"W\",\n",
    "    \"H\",\n",
    "    \"T\",\n",
    "    \"G\",\n",
    "    \"F\",\n",
    "    \"N\",\n",
    "    \"V\",\n",
    "    \"I\",\n",
    "    \"O\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"U\", \n",
    "    \"Q\",\n",
    "    \"X\",\n",
    "]\n",
    "\n",
    "VALID_GEN = [\"F\", \"M\", \"Other\", \"Unknown\"]\n",
    "\n",
    "VALID_ENTITIES = [\"I\", \"O\"]\n",
    "\n",
    "VALID_DESC_FLAGS = [\"S\", \"T\"]\n",
    "\n",
    "VALID_ENROLLS = [\"E\", \"N\", \"O\"]\n",
    "\n",
    "# Some utilities                                                                              \n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_to_int(x):\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Read data                                                                                   \n",
    "df = pd.read_csv(INP_FILE, on_bad_lines = \"skip\", low_memory=False)\n",
    "    \n",
    "# Clean categorical variables                                                                 \n",
    "df[\"nppes_provider_mi\"] = df[\"nppes_provider_mi\"].fillna(\"Unknown\")\n",
    "df[\"nppes_provider_mi\"] = df[\"nppes_provider_mi\"].apply(\n",
    "    lambda x: x if x in VALID_PROVIDER_MI else \"Unknown\"\n",
    ")\n",
    "\n",
    "df[\"nppes_credentials\"] = df[\"nppes_credentials\"].fillna(\"Unknown\")\n",
    "df[\"nppes_credentials\"] = df[\"nppes_credentials\"].apply(\n",
    "    lambda x: str(x).replace(\".\", \"\")\n",
    ")\n",
    "\n",
    "cred_hash = {\n",
    "    \"MEDICAL DOCTOR\": \"MD\",\n",
    "    \"NURSE PRACTITIONER\": \"NP\",\n",
    "}\n",
    "df[\"nppes_credentials\"] = df[\"nppes_credentials\"].apply(\n",
    "    lambda x: cred_hash[x] if x in cred_hash else x,\n",
    ")\n",
    "\n",
    "df[\"nppes_provider_gender\"] = df[\"nppes_provider_gender\"].fillna(\"Unknown\")\n",
    "df[\"nppes_provider_gender\"] = df[\"nppes_provider_gender\"].apply(\n",
    "    lambda x: x if x in VALID_GEN else \"Other\",\n",
    ")\n",
    "\n",
    "df[\"nppes_entity_code\"] = df[\"nppes_entity_code\"].apply(\n",
    "    lambda x: x if x in VALID_ENTITIES else \"Unknown\",\n",
    ")\n",
    "\n",
    "df[\"nppes_provider_zip5\"] = df[\"nppes_provider_zip5\"].fillna(\"Unknown\")\n",
    "\n",
    "df[\"nppes_provider_country\"] = df[\"nppes_provider_country\"].apply(\n",
    "    lambda x: \"US\" if x == \"US\" else \"Other\",\n",
    ")\n",
    "\n",
    "df[\"description_flag\"] = df[\"description_flag\"].apply(\n",
    "    lambda x: x if x in VALID_DESC_FLAGS else \"Unknown\",\n",
    ")\n",
    "\n",
    "df[\"medicare_prvdr_enroll_status\"] = df[\"medicare_prvdr_enroll_status\"].apply(\n",
    "    lambda x: x if x in VALID_ENROLLS else \"Unknown\",\n",
    ")\n",
    "\n",
    "\n",
    "# Treat missing beneficiary count as it cannot be zero                                        \n",
    "df[\"bene_count\"] = df[\"bene_count\"].apply(\n",
    "    convert_to_int\n",
    ").fillna(-1)\n",
    "\n",
    "tmp_df = df.groupby(\n",
    "    \"specialty_description\", as_index=False,\n",
    ")[\"bene_count\"].mean()\n",
    "\n",
    "bene_count_hash = dict(\n",
    "    zip(\n",
    "        tmp_df[\"specialty_description\"],\n",
    "        tmp_df[\"bene_count\"],\n",
    "    )\n",
    ")\n",
    "df[\"bene_count\"] = df.apply(\n",
    "    lambda x: x[\"bene_count\"] if x[\n",
    "        \"bene_count\"\n",
    "    ] > 0 else bene_count_hash[\n",
    "        x[\"specialty_description\"]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Treat continuous variables                                                                  \n",
    "for item in CON_VARS:\n",
    "    df[item] = df[item].apply(\n",
    "        convert_to_float\n",
    "    ).fillna(0.0)\n",
    "\n",
    "# Filter out invalid states                                                                   \n",
    "df  = df[\n",
    "    ~df[\"nppes_provider_state\"].isin(\n",
    "        [\"XX\", \"E\", \"N\", \"S\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Output                                                                                      \n",
    "df.to_csv(OUT_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features\n",
    "\n",
    "We then generate features. The categorical features are encoded using teh average value of a few important variables in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input                                                                                       \n",
    "INP_FILE = \"cleaned_medicare_data.csv\"\n",
    "OUT_FILE  = \"medicare_features.csv\"\n",
    "\n",
    "# Set some parameters                                                                         \n",
    "CAT_VARS = [\n",
    "    \"nppes_provider_mi\",\n",
    "    #\"nppes_credentials\", # This is rather messy, so ignoring it.                             \n",
    "    \"nppes_provider_gender\",\n",
    "    \"nppes_entity_code\",\n",
    "    \"nppes_provider_city\",\n",
    "    \"nppes_provider_zip5\",\n",
    "    #\"nppes_provider_country\", # Almost all cases are US                                      \n",
    "    \"specialty_description\",\n",
    "    \"medicare_prvdr_enroll_status\",\n",
    "    \"nppes_provider_state\",\n",
    "]\n",
    "\n",
    "CON_VARS = [\n",
    "    \"total_claim_count\",\n",
    "    \"total_30_day_fill_count\",\n",
    "    \"total_drug_cost\",\n",
    "    \"total_day_supply\",\n",
    "    \"bene_count\",\n",
    "    \"total_claim_count_ge65\",\n",
    "    \"total_30_day_fill_count_ge65\",\n",
    "    \"total_drug_cost_ge65\",\n",
    "    \"total_day_supply_ge65\",\n",
    "    \"bene_count_ge65\",\n",
    "    \"brand_claim_count\",\n",
    "    \"brand_drug_cost\",\n",
    "    \"generic_claim_count\",\n",
    "    \"generic_drug_cost\",\n",
    "    \"other_claim_count\",\n",
    "    \"other_drug_cost\",\n",
    "    \"mapd_claim_count\",\n",
    "    \"mapd_drug_cost\",\n",
    "    \"pdp_claim_count\",\n",
    "    \"pdp_drug_cost\",\n",
    "    \"lis_claim_count\",\n",
    "    \"lis_drug_cost\",\n",
    "    \"nonlis_claim_count\",\n",
    "    \"nonlis_drug_cost\",\n",
    "    \"opioid_claim_count\",\n",
    "    \"opioid_drug_cost\",\n",
    "    \"opioid_day_supply\",\n",
    "    \"opioid_bene_count\",\n",
    "    \"antibiotic_claim_count\",\n",
    "    \"antibiotic_drug_cost\",\n",
    "    \"antibiotic_bene_count\",\n",
    "    \"hrm_claim_count_ge65\",\n",
    "    \"hrm_drug_cost_ge65\",\n",
    "    \"hrm_bene_count_ge65\",\n",
    "    \"antipsych_claim_count_ge65\",\n",
    "    \"antipsych_drug_cost_ge65\",\n",
    "    \"antipsych_bene_count_ge65\",\n",
    "    \"average_age_of_beneficiaries\",\n",
    "    \"beneficiary_age_less_65_count\",\n",
    "    \"beneficiary_age_65_74_count\",\n",
    "    \"beneficiary_age_75_84_count\",\n",
    "    \"beneficiary_age_greater_84_count\",\n",
    "    \"beneficiary_female_count\",\n",
    "    \"beneficiary_male_count\",\n",
    "    \"beneficiary_race_white_count\",\n",
    "    \"beneficiary_race_black_count\",\n",
    "    \"beneficiary_race_asian_pi_count\",\n",
    "    \"beneficiary_race_hispanic_count\",\n",
    "    \"beneficiary_race_nat_ind_count\",\n",
    "    \"beneficiary_race_other_count\",\n",
    "    \"beneficiary_nondual_count\",\n",
    "    \"beneficiary_dual_count\",\n",
    "    \"beneficiary_average_risk_score\",\n",
    "]\n",
    "\n",
    "# Read and clean data                                                                         \n",
    "df = pd.read_csv(INP_FILE, low_memory=False)\n",
    "\n",
    "# Embed categorical features                                                                  \n",
    "embedded_cat_features = []\n",
    "for item in CAT_VARS:\n",
    "    tmp_df = df.groupby(item, as_index=False).agg(\n",
    "        {\n",
    "            \"opioid_claim_count\": \"mean\",\n",
    "            \"opioid_drug_cost\": \"mean\",\n",
    "            \"opioid_day_supply\": \"mean\",\n",
    "            \"opioid_bene_count\": \"mean\",\n",
    "            \"opioid_prescriber_rate\": \"mean\",\n",
    "        }\n",
    "    ).rename(\n",
    "        columns={\n",
    "            \"opioid_claim_count\": \"%s_opioid_claim_count\" % item,\n",
    "            \"opioid_drug_cost\": \"%s_opioid_drug_cost\" % item,\n",
    "            \"opioid_day_supply\": \"%s_opioid_day_supply\" % item,\n",
    "            \"opioid_bene_count\": \"%s_opioid_bene_count\" % item,\n",
    "            \"opioid_prescriber_rate\": \"%s_opioid_prescriber_rate\" % item,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df = df.merge(tmp_df, how=\"left\", on=item)\n",
    "\n",
    "    embedded_cat_features += [\n",
    "        \"%s_opioid_claim_count\" % item,\n",
    "        \"%s_opioid_drug_cost\" % item,\n",
    "        \"%s_opioid_day_supply\" % item,\n",
    "        \"%s_opioid_bene_count\" % item,\n",
    "        \"%s_opioid_prescriber_rate\" % item,\n",
    "    ]\n",
    "\n",
    "# Drop unembedded categorical variables and some others                                       \n",
    "df = df[[\"npi\"] + CON_VARS + embedded_cat_features]\n",
    "\n",
    "# Write features file                                                                         \n",
    "df.to_csv(OUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Once the features are generated, we can implemented the above-mentioned feature selection algorithm. We start by importing some libraries, setting some parameters, and loading the features into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from qci_client import QciClient\n",
    "from qci_client import load_json_file\n",
    "\n",
    "\n",
    "# Define some parameters\n",
    "FEATURES_FILE = \"medicare_features.csv\"\n",
    "REDUCED_DIM = 10\n",
    "\n",
    "# Read features\n",
    "df = pd.read_csv(FEATURES_FILE, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the feature names and get the total count of features in the dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(set(df.columns) - {\"npi\"})\n",
    "\n",
    "orig_dim = len(feature_names)\n",
    "\n",
    "print(\n",
    "    \"Original dimension is %d; reduced dimension will be %d\" % (\n",
    "        orig_dim,\n",
    "        REDUCED_DIM,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now create the objective matrix $C$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the objective matrix\n",
    "X = np.array(df[feature_names])\n",
    "\n",
    "C = abs(np.corrcoef(X, rowvar=False))\n",
    "\n",
    "# Make correlation symmetric to machine precision\n",
    "C = 0.5 * (C + C.transpose())\n",
    "\n",
    "objective = C - np.eye(orig_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the constraint matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the constraint\n",
    "cons_lhs = np.zeros(shape=(orig_dim), dtype=np.float64)\n",
    "for i in range(orig_dim):\n",
    "    cons_lhs[i] = 1.0\n",
    "\n",
    "cons_lhs = np.array([cons_lhs])\n",
    "cons_rhs = [REDUCED_DIM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now solve the above quadratic binary problem using QCi's Dirac-1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cons_lhs)\n",
    "# Create json objects                                                                     \n",
    "objective_json = {}\n",
    "objective_json[\"data\"] = []\n",
    "for i in range(orig_dim):\n",
    "    for j in range(orig_dim):\n",
    "        if objective[i][j] == 0:\n",
    "            continue\n",
    "        objective_json[\"data\"].append(\n",
    "            {\"i\": i, \"j\": j, \"val\": objective[i][j]}\n",
    "        )\n",
    "        \n",
    "objective_json[\"file_name\"] = \"objective_tutorial.json\"\n",
    "objective_json[\"num_variables\"] = orig_dim\n",
    "objective_json[\"file_type\"] = \"objective\"\n",
    "\n",
    "# Create constraint jasons\n",
    "constraint_json = {}\n",
    "constraint_json[\"data\"] = []\n",
    "for i in range(cons_lhs.shape[0]):\n",
    "    for j in range(cons_lhs.shape[1]):\n",
    "        if cons_lhs[i][j] == 0:\n",
    "            continue\n",
    "        constraint_json[\"data\"].append(\n",
    "            {\"i\": i, \"j\": j, \"val\": cons_lhs[i][j]}\n",
    "        )\n",
    "        \n",
    "constraint_json[\"file_name\"] = \"constraints_tutorial.json\"\n",
    "constraint_json[\"num_constraints\"] = len(cons_rhs)\n",
    "constraint_json[\"num_variables\"] = orig_dim\n",
    "constraint_json[\"file_type\"] = \"constraints\"\n",
    "\n",
    "rhs_json = {}\n",
    "rhs_json[\"data\"] = cons_rhs\n",
    "rhs_json[\"file_name\"] = \"rhs_tutorial.json\"\n",
    "rhs_json[\"num_constraints\"] = len(cons_rhs)\n",
    "rhs_json[\"file_type\"] = \"rhs\"\n",
    "    \n",
    "# Solve the optimizzation problem\n",
    "qci = QciClient()\n",
    "\n",
    "response_json = qci.upload_file(objective_json)\n",
    "objective_file_id = response_json[\"file_id\"]\n",
    "\n",
    "response_json = qci.upload_file(constraint_json)\n",
    "constraint_file_id = response_json[\"file_id\"]\n",
    "\n",
    "response_json = qci.upload_file(rhs_json)\n",
    "rhs_file_id = response_json[\"file_id\"]\n",
    "\n",
    "# Setup job json\n",
    "job_json = {\n",
    "    \"job_name\": \"tutorial_eqc1\",\n",
    "    \"job_tags\": [\"tutorial_eqc1\"],\n",
    "    \"params\": {\n",
    "        \"sampler_type\": \"csample\", #\"eqc1\",                                             \n",
    "        \"n_samples\": 20,\n",
    "        \"alpha\": 1.0,\n",
    "    },\n",
    "}\n",
    "job_json[\"constraints_file_id\"] = constraint_file_id\n",
    "job_json[\"objective_file_id\"] = objective_file_id\n",
    "job_json[\"rhs_file_id\"] = rhs_file_id\n",
    "\n",
    "# Run the job\n",
    "job_response_json = qci.process_job(\n",
    "    job_body=job_json, job_type=\"sample-constraint\",\n",
    ")\n",
    "\n",
    "print(job_response_json)\n",
    "\n",
    "if job_response_json[\"job_info\"][\"details\"][\"status\"] == \"COMPLETED\":\n",
    "    results = job_response_json[\"results\"]\n",
    "    energies = results[\"energies\"]\n",
    "    samples = results[\"samples\"]\n",
    "    is_feasibles = results[\"is_feasible\"]\n",
    "else:\n",
    "    assert False, job_response_json[\"job_info\"][\"results\"][\"error\"]\n",
    "        \n",
    "if True:\n",
    "    print(\"Energies:\", energies)        \n",
    "\n",
    "# Pick a feasible solution with lowest energy                                             \n",
    "# The sample solutions are sorted by energy                                               \n",
    "sol = None\n",
    "for i, item in enumerate(samples):\n",
    "    sol = item\n",
    "    is_feasible = is_feasibles[i]\n",
    "\n",
    "    if is_feasible:\n",
    "        break\n",
    "\n",
    "if not is_feasible:\n",
    "    print(\"Solution is not feasible!\")\n",
    "\n",
    "assert sol is not None, \"No feasible solution found!\"\n",
    "\n",
    "assert len(sol) == orig_dim, \"Inconsistent solution size!\"\n",
    "\n",
    "assert sum(sol) == REDUCED_DIM, \"Solution is not feasible!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can print the list of selected variables,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vars = []\n",
    "for i in range(orig_dim):\n",
    "    if sol[i] > 0:\n",
    "        selected_vars.append(feature_names[i])\n",
    "\n",
    "print(selected_vars)      "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
